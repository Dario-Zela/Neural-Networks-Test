{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dario-Zela/Neural-Networks-Test/blob/main/Self_made_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqx3nFC2JDRU"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-PXp982JTnK"
      },
      "outputs": [],
      "source": [
        "class Value:\n",
        "\n",
        "  def __init__(self, value, children = (), label = \"\"):\n",
        "    self.data = np.atleast_2d(np.array(value, 'float64'))\n",
        "    self.grad = np.zeros_like(self.data)\n",
        "    self._backward = lambda : None\n",
        "    self._prev = set(children)\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    s = f\"Value {self.label}: Stores {self.data} | Gradients {self.grad}\"\n",
        "    if len(self._prev) != 0:\n",
        "      s += \"\\nContains : {\"\n",
        "\n",
        "    for child in self._prev:\n",
        "      s += f\"\\n\\t{child}\"\n",
        "\n",
        "    if len(self._prev) != 0:\n",
        "      s += \"\\n\\t}\"\n",
        "\n",
        "    return s\n",
        "\n",
        "  def st(self):\n",
        "    return f\"Value {self.label}: Stores {self.data} | Gradients {self.grad}\"\n",
        "\n",
        "\n",
        "  def __add__(self, other):\n",
        "    if not isinstance(other, Value):\n",
        "      other = Value(other)\n",
        "      \n",
        "    out = Value(np.add(self.data, other.data), (self, other), label= f\"{self.label}+{other.label}\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += out.grad\n",
        "      other.grad += out.grad\n",
        "      #print(\"add\")\n",
        "      #print(self.grad)\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __mul__(self, other):\n",
        "    if not isinstance(other, Value):\n",
        "      other = Value(other)\n",
        "    out = Value(np.matmul(other.data, self.data.transpose()), (self, other), label= f\"{self.label}*{other.label}\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad += np.matmul(out.grad.transpose(), other.data)\n",
        "      other.grad += np.matmul(out.grad, self.data)\n",
        "      #print(\"mul\")\n",
        "      #print(self.grad)\n",
        "    \n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def tanh(self):\n",
        "    x = self.data\n",
        "    t = np.tanh(self.data)\n",
        "\n",
        "    out = Value(t, (self, ), label= f\"tanh({self.label})\")\n",
        "\n",
        "    def _backward():\n",
        "      self.grad +=  (1 - t ** 2) * out.grad\n",
        "      #print(\"tanh\")\n",
        "      #print(x)\n",
        "      #print(self.grad) \n",
        "    \n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def __sub__(self, other):\n",
        "    if not isinstance(other, Value):\n",
        "      other = Value(other)\n",
        "      \n",
        "    out = Value(np.subtract(self.data, other.data), (self, other), label= f\"{self.label}-{other.label}\")\n",
        "\n",
        "    def _backward():\n",
        "      #print(\"sub\")\n",
        "      #print(self.grad)\n",
        "      self.grad += out.grad\n",
        "      other.grad -= out.grad\n",
        "      #print(out.grad)\n",
        "\n",
        "    out._backward = _backward\n",
        "    return out\n",
        "\n",
        "  def backward(self):\n",
        "    topo = []\n",
        "    visited = set()\n",
        "\n",
        "    def build_topo(v):\n",
        "      if v not in visited:\n",
        "        visited.add(v)\n",
        "        for child in v._prev:\n",
        "          build_topo(child)\n",
        "        topo.append(v)\n",
        "\n",
        "    build_topo(self)\n",
        "\n",
        "    self.grad = 1.0\n",
        "    for node in reversed(topo):\n",
        "      node._backward()\n",
        "\n",
        "  def square(self):\n",
        "    out = Value(self.data ** 2, (self, ), label= f\"({self.label})**2\")\n",
        "    def _backward():\n",
        "      #print(\"square\")\n",
        "      self.grad += 2 * out.grad * self.data\n",
        "      #print(self.grad)\n",
        "\n",
        "    out._backward = _backward\n",
        "\n",
        "    return out\n",
        "\n",
        "  def zero_grad(self):\n",
        "    self.grad = 0.0\n",
        "    for child in self._prev:\n",
        "      child.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXf0A1pCV75C"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "  def __init__(self, nin, nout):\n",
        "    self.w = Value(np.random.rand(nout, nin), label = f\"w{nin}{nout}\")\n",
        "    self.b = Value(np.random.rand(nout), label = f\"b{nin}{nout}\")\n",
        "\n",
        "  def __call__(self, x):\n",
        "    a = (self.w * x + self.b).tanh()\n",
        "    return a\n",
        "    \n",
        "\n",
        "  def params(self):\n",
        "    return [self.w, self.b]\n",
        "\n",
        "class MLP:\n",
        "  def __init__(self, nin, nouts):\n",
        "    sz = [nin] + nouts\n",
        "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
        "\n",
        "  def __call__(self, x):\n",
        "    x = np.atleast_2d(np.array(x))\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "  \n",
        "  def params(self):\n",
        "    return [p for layer in self.layers for p in layer.params()]\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    for p in self.params():\n",
        "      p.zero_grad()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afs-VcSlMuK2"
      },
      "outputs": [],
      "source": [
        "ys = [1,-1,-1,1]\n",
        "xs = [\n",
        "    [2, 3, -1],\n",
        "    [3, -1, 0.5],\n",
        "    [0.5,1,1],\n",
        "    [1,-1,-1]\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = MLP(3, [4,3,1])"
      ],
      "metadata": {
        "id": "yp-h_1fSI3nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQhvtH--ibk9",
        "outputId": "7da70e5d-a669-467b-b230-597ffb85fdf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [[0.01922368]]\n",
            "1 [[0.01896089]]\n",
            "2 [[0.01870502]]\n",
            "3 [[0.0184558]]\n",
            "4 [[0.01821297]]\n",
            "5 [[0.0179763]]\n",
            "6 [[0.01774556]]\n",
            "7 [[0.01752053]]\n",
            "8 [[0.017301]]\n",
            "9 [[0.01708678]]\n",
            "10 [[0.01687768]]\n",
            "11 [[0.01667351]]\n",
            "12 [[0.01647412]]\n",
            "13 [[0.01627933]]\n",
            "14 [[0.01608899]]\n",
            "15 [[0.01590295]]\n",
            "16 [[0.01572107]]\n",
            "17 [[0.0155432]]\n",
            "18 [[0.01536923]]\n",
            "19 [[0.01519903]]\n",
            "20 [[0.01503247]]\n",
            "21 [[0.01486944]]\n",
            "22 [[0.01470983]]\n",
            "23 [[0.01455354]]\n",
            "24 [[0.01440047]]\n",
            "25 [[0.01425051]]\n",
            "26 [[0.01410357]]\n",
            "27 [[0.01395957]]\n",
            "28 [[0.01381842]]\n",
            "29 [[0.01368004]]\n",
            "30 [[0.01354434]]\n",
            "31 [[0.01341125]]\n",
            "32 [[0.0132807]]\n",
            "33 [[0.01315261]]\n",
            "34 [[0.01302691]]\n",
            "35 [[0.01290355]]\n",
            "36 [[0.01278245]]\n",
            "37 [[0.01266356]]\n",
            "38 [[0.01254682]]\n",
            "39 [[0.01243216]]\n",
            "40 [[0.01231954]]\n",
            "41 [[0.0122089]]\n",
            "42 [[0.01210019]]\n",
            "43 [[0.01199336]]\n",
            "44 [[0.01188836]]\n",
            "45 [[0.01178515]]\n",
            "46 [[0.01168368]]\n",
            "47 [[0.01158391]]\n",
            "48 [[0.01148579]]\n",
            "49 [[0.01138929]]\n",
            "50 [[0.01129437]]\n",
            "51 [[0.01120098]]\n",
            "52 [[0.0111091]]\n",
            "53 [[0.01101868]]\n",
            "54 [[0.01092969]]\n",
            "55 [[0.0108421]]\n",
            "56 [[0.01075588]]\n",
            "57 [[0.01067099]]\n",
            "58 [[0.01058741]]\n",
            "59 [[0.0105051]]\n",
            "60 [[0.01042404]]\n",
            "61 [[0.01034419]]\n",
            "62 [[0.01026554]]\n",
            "63 [[0.01018805]]\n",
            "64 [[0.0101117]]\n",
            "65 [[0.01003646]]\n",
            "66 [[0.00996232]]\n",
            "67 [[0.00988924]]\n",
            "68 [[0.00981721]]\n",
            "69 [[0.0097462]]\n",
            "70 [[0.00967619]]\n",
            "71 [[0.00960716]]\n",
            "72 [[0.00953909]]\n",
            "73 [[0.00947196]]\n",
            "74 [[0.00940575]]\n",
            "75 [[0.00934045]]\n",
            "76 [[0.00927602]]\n",
            "77 [[0.00921247]]\n",
            "78 [[0.00914976]]\n",
            "79 [[0.00908789]]\n",
            "80 [[0.00902684]]\n",
            "81 [[0.00896658]]\n",
            "82 [[0.00890711]]\n",
            "83 [[0.00884841]]\n",
            "84 [[0.00879046]]\n",
            "85 [[0.00873326]]\n",
            "86 [[0.00867678]]\n",
            "87 [[0.00862101]]\n",
            "88 [[0.00856595]]\n",
            "89 [[0.00851157]]\n",
            "90 [[0.00845787]]\n",
            "91 [[0.00840482]]\n",
            "92 [[0.00835243]]\n",
            "93 [[0.00830068]]\n",
            "94 [[0.00824955]]\n",
            "95 [[0.00819903]]\n",
            "96 [[0.00814912]]\n",
            "97 [[0.00809981]]\n",
            "98 [[0.00805108]]\n",
            "99 [[0.00800292]]\n",
            "100 [[0.00795532]]\n",
            "101 [[0.00790828]]\n",
            "102 [[0.00786178]]\n",
            "103 [[0.00781582]]\n",
            "104 [[0.00777038]]\n",
            "105 [[0.00772545]]\n",
            "106 [[0.00768104]]\n",
            "107 [[0.00763712]]\n",
            "108 [[0.0075937]]\n",
            "109 [[0.00755076]]\n",
            "110 [[0.00750829]]\n",
            "111 [[0.00746629]]\n",
            "112 [[0.00742475]]\n",
            "113 [[0.00738366]]\n",
            "114 [[0.00734302]]\n",
            "115 [[0.00730282]]\n",
            "116 [[0.00726304]]\n",
            "117 [[0.00722369]]\n",
            "118 [[0.00718476]]\n",
            "119 [[0.00714624]]\n",
            "120 [[0.00710812]]\n",
            "121 [[0.0070704]]\n",
            "122 [[0.00703307]]\n",
            "123 [[0.00699613]]\n",
            "124 [[0.00695956]]\n",
            "125 [[0.00692338]]\n",
            "126 [[0.00688756]]\n",
            "127 [[0.0068521]]\n",
            "128 [[0.006817]]\n",
            "129 [[0.00678225]]\n",
            "130 [[0.00674785]]\n",
            "131 [[0.00671379]]\n",
            "132 [[0.00668007]]\n",
            "133 [[0.00664668]]\n",
            "134 [[0.00661362]]\n",
            "135 [[0.00658087]]\n",
            "136 [[0.00654845]]\n",
            "137 [[0.00651634]]\n",
            "138 [[0.00648454]]\n",
            "139 [[0.00645304]]\n",
            "140 [[0.00642184]]\n",
            "141 [[0.00639094]]\n",
            "142 [[0.00636033]]\n",
            "143 [[0.00633]]\n",
            "144 [[0.00629996]]\n",
            "145 [[0.0062702]]\n",
            "146 [[0.00624072]]\n",
            "147 [[0.0062115]]\n",
            "148 [[0.00618256]]\n",
            "149 [[0.00615388]]\n",
            "150 [[0.00612546]]\n",
            "151 [[0.00609729]]\n",
            "152 [[0.00606938]]\n",
            "153 [[0.00604173]]\n",
            "154 [[0.00601431]]\n",
            "155 [[0.00598715]]\n",
            "156 [[0.00596022]]\n",
            "157 [[0.00593353]]\n",
            "158 [[0.00590708]]\n",
            "159 [[0.00588085]]\n",
            "160 [[0.00585486]]\n",
            "161 [[0.00582909]]\n",
            "162 [[0.00580354]]\n",
            "163 [[0.00577822]]\n",
            "164 [[0.0057531]]\n",
            "165 [[0.00572821]]\n",
            "166 [[0.00570352]]\n",
            "167 [[0.00567905]]\n",
            "168 [[0.00565478]]\n",
            "169 [[0.00563071]]\n",
            "170 [[0.00560685]]\n",
            "171 [[0.00558318]]\n",
            "172 [[0.00555971]]\n",
            "173 [[0.00553643]]\n",
            "174 [[0.00551334]]\n",
            "175 [[0.00549045]]\n",
            "176 [[0.00546774]]\n",
            "177 [[0.00544521]]\n",
            "178 [[0.00542287]]\n",
            "179 [[0.0054007]]\n",
            "180 [[0.00537872]]\n",
            "181 [[0.00535691]]\n",
            "182 [[0.00533527]]\n",
            "183 [[0.00531381]]\n",
            "184 [[0.00529251]]\n",
            "185 [[0.00527138]]\n",
            "186 [[0.00525042]]\n",
            "187 [[0.00522962]]\n",
            "188 [[0.00520899]]\n",
            "189 [[0.00518851]]\n",
            "190 [[0.00516819]]\n",
            "191 [[0.00514803]]\n",
            "192 [[0.00512802]]\n",
            "193 [[0.00510817]]\n",
            "194 [[0.00508846]]\n",
            "195 [[0.00506891]]\n",
            "196 [[0.0050495]]\n",
            "197 [[0.00503024]]\n",
            "198 [[0.00501113]]\n",
            "199 [[0.00499215]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(200):\n",
        "\n",
        "  ypred = [n(x) for x in xs]\n",
        "\n",
        "  loss = Value([0])\n",
        "  for j in ((yout - ygt).square() for ygt, yout in zip(ys, ypred)):\n",
        "    loss += j\n",
        "\n",
        "  n.zero_grad()\n",
        "  loss.backward()\n",
        "\n",
        "  for p in n.params():\n",
        "    p.data += -0.05 * p.grad\n",
        "\n",
        "  print(i, loss.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n([1,-1,-1]).st()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oD0bX91_6O2P",
        "outputId": "8461d310-2dce-46a7-cf82-faadae4a6aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Value tanh(w31*tanh(w43*tanh(w34*+b34)+b43)+b31): Stores [[0.96973147]] | Gradients [[0.]]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 687
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ5n5fLcUbm3KU2zO/jvPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}